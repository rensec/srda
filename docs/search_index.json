[["index.html", "Social Research in the Digital Age (Utrecht University) About Usage", " Social Research in the Digital Age (Utrecht University) Rense Corten About This page contains tutorials for the Utrecht University course “Social Research in the Digital Age”. Usage To use these tutorials, we suggest that you: Open a new R script; Copy-paste the code from the tutorials into your R script as you read along; Run the code from your script while reading along, to verify that your results are the same; Afterwards, save your R script under an informative name so that you can refer to it and reuse it later. A big part of effective data analysis is realizing that you’ve solved particular problems before and being able to find and recycle your earlier code! "],["a-very-short-introduction-to-r.html", "1 A Very Short Introduction to R 1.1 Introduction 1.2 Using RStudio 1.3 Running code from the R script 1.4 Using objects: Assigning values to names 1.5 Comments 1.6 Installing Packages 1.7 The working directory 1.8 Interacting with the Environment 1.9 Back to writing code 1.10 Data structures 1.11 Getting data into R 1.12 Saving data 1.13 Functions 1.14 Getting help: viewing and interpreting function documentation", " 1 A Very Short Introduction to R 1.1 Introduction Welcome to the first week of Part I - Introduction to R! The aim of this practical is to introduce you to the R programming language as well as RStudio and some of its functionalities. You will also practice using some basic functions and operations. The contents of this practical are based on materials developed by Kyle Lang (https://github.com/kylelang/Introduction-to-R) as well as Kasper Welbers, Wouter van Atteveldt, Philipp Masur, and Paulina Pankowska (https://github.com/ccs-amsterdam/r-course-material). 1.1.1 What is R and why should you learn it? R is an open-source statistical software language, that is currently among the most popular languages for data science. In comparison to other popular software packages in social scientific research, such as SPSS and Stata, R has several notable advantages: R is a programming language, which makes it much more versatile. While R focuses on statistical analysis at heart, it facilitates a wide-range of features, and virtually any tool for data science can be implemented. The range of things you can do with R is constantly being updated. R is open-source, meaning that anyone can contribute to its development. In particular, people can develop new packages, that can easily and safely be installed from within R with a single command. Since many scholars and industry professionals use R, it is likely that any cutting-edge and bleeding-edge techniques that you are interested in are already available. You can think of it as an app-store for all your data-science needs! R is free. While for students this is not yet a big deal due to free or cheap student and university licences, this can be a big plus in the commercial sector. Especially for small businesses and free-lancers. 1.2 Using RStudio Once you have installed R and RStudio, you can start by launching RStudio. If everything was installed correctly, RStudio will automatically launch R as well. The first time you open RStudio, you will likely see three separate windows. The first thing you want to do is open an R Script to work in. To do so, go to the toolbar and select File -&gt; New File -&gt; R Script. You will now see four windows split evenly over the four corners of your screen: In the top-left you have the text editor for the file that you are working in. This will most of the time be an R script or RMarkdown file. In the top-right you can see the data and values that you are currently working with (environment) or view your history of input. In the bottom-left you have the console, which is where you can enter and run code, and view the output. If you run code from your R script, it will also be executed in this console. In the bottom-right you can browse through files on your computer, view help for functions, or view visualizations. While you can directly enter code into your console (bottom-left), you should, generally, always work with R scripts (top-left). This allows you to keep track of what you are doing and save every step. There are a few exceptions, which we will cover later. Note: You can also view your R script in a separate window by clicking on the icon with the small upward-pointing arrow in the top-left corner of the script editor. This is similar to the setup in, say, SPSS or Stata, and is especially useful if you have multiple monitors or a large screen. 1.3 Running code from the R script Copy and paste the following example code into your R Script. For now, don’t bother understanding the syntax itself. Just focus on running it. 3 + 3 2 * 5 (6 / 2) * 3 &quot;some text&quot; &quot;some more text&quot; sum(1,2,3,4,5) ## using a function You can run parts of the code in an R script by pressing Ctrl + Enter (on mac this is command + Enter). This can be done in two ways: If you select a piece of text (so that it is highlighted) you can press Ctrl + Enter to run the selection. For example, select the first three lines (the three mathematical operations) and press Ctrl + Enter. If you haven’t made a selection, but your text cursor is in the editor, you can press Ctrl + Enter to run the line where the cursor is at. This will also move the cursor to the next line, so you can walk through the code from top to bottom, running each line. Try starting on the first line, and pressing Ctrl + Enter six times, to run each line separately. 1.4 Using objects: Assigning values to names To do anything useful, we need to create objects that hold data. When running the example code, you saw that R automatically evaluates expressions. The calculation 3+3 evaluates to 6, and 2*5 evaluates to 10. You also saw that the function sum(1,2,3,4,5) evaluates to 15 (the sum of the numbers). For now, one more thing that you need to know about the R syntax is how values can be assigned to objects via the ‘assignment’ operator. In plain terms, assignment is how you make R remember things by assigning them a name. This works the same way for all sorts of values, from single numbers to entire datasets. You can choose whether you prefer the equal sign (=) or the arrow (&lt;-) for assignment. x = 2 x2 &lt;- 33.33 y &lt;- 4 &quot;Bob&quot; -&gt; z Here we have, among other things, remembered the number 2 as x and the text “some text” as y. If you are working in RStudio (which you should), you can now also see these names and values in the topright window, under the “Environment” tab. Evaluating an object name without assignment prints the value of that object (i.e., use the names to retrieve the values) y x z We can also use these values in new commands. x * 5 Note The assigned values can also be named objects w &lt;- y w Object names must begin with a letter my1X &lt;- pi my1X # 1X &lt;- pi # this will not work :( 1.4.1 PRACTICE PROBLEM 1 Create an object named test and assign the sum of 10 and 15 to it. # SOLUTION test &lt;- 10+15 1.5 Comments Comments are a useful way to annotate or document your code, so you and others can more easily understand it and know what is going on. The comment character in R is ‘#’; each commented line must be preceded by a ‘#’ symbol as there are no block comments in R. Comments are not evaluated when you run your code 1.6 Installing Packages Packages are collections of R functions (which we’ll talk about more later. For now it is enough to know that they are a set of statements organized together to perform a specific tasks; they are similar to commands in Stata or SPSS), data, and compiled code in a well-defined format, created to add specific functionality. There are 10,000+ user contributed packages and growing. There are a set of standard (or base) packages which are considered part of the R source code and automatically available as part of your R installation. Base packages contain the basic functions that allow R to work, and enable standard statistical and graphical functions on datasets. Throughout our course you will need to install various packages, for example, for web scraping or network analysis. The standard way to install packages is using the following command: install.packages(&quot;psych&quot;) This command will install the package ‘psych’ from the Comprehensive R Archive Network (CRAN). CRAN is a network of servers around the world that store identical, up-to-date, versions of code and documentation for R. There are also ways to install packages from other places, which we don’t cover here for simplicity. 1.6.1 PRACTICE PROBLEM 2 Use install.packages() to install the following packages in the default location (i.e., don’t specify anything for the ‘lib’ argument): - ggplot2 - dplyr - haven # SOLUTION #install.packages(&quot;ggplot2&quot;) # install.packages(&quot;dplyr&quot;) # install.packages(&quot;haven&quot;) Much like any other software, we only need to install a package once on our computer (unless we update R), but we need to load the package each time we open R and want to use it. library(psych) While we recommended that you generally run commands from scripts rather than from the console, “install.packages()” may be an exception: as you typically need to run this only once per package, it is often more practical to run it directly from the console. 1.7 The working directory Every R session is associated with a ‘working directory’. The working directory is the directory wherein R will root its navigation when reading or writing data objects to or from disk. Find the current working directory getwd() Change the current working directory setwd(&quot;/Users/corte101/Teaching&quot;) # obviously this will not work for you, unless you are me! getwd() Note: Although opinions on this differ, this may be another exception to the rule that you should run commands from scripts rather than from the console. Setting the working directory is somwthing you may want to do outside your script, as it is a setting that is specific to your computer and your file structure, and you may not want your script to depend on that to work correctly. 1.7.1 PRACTICE PROBLEM 3 Use the setwd() to change your working directory to the directory in which this script is saved. # SOLUTION setwd() setwd(&quot;/Users/panko001/Desktop&quot;) #This serves as an example and will not work since where I have it saved it different from where you have it saved 1.8 Interacting with the Environment The ‘environment’ is a loosely organized set of all the objects that R currently has stored in working memory. We can check the contents of the current environment using a command: ls() …but you can also see the content of your environment in the top-right window of RStudio, under the “Environment” tab. And remove an object from the environment rm(x) ls() Or completely clear the enviroment rm(list = ls(all = TRUE)) ls() 1.8.1 PRACTICE PROBLEM 4 Create two objects and assign any value to them. Then use rm() to remove one of the objects. # SOLUTION value1 &lt;- 4 value2 &lt;- 73 rm(value1, value2) 1.9 Back to writing code 1.9.1 Mathematical Operators Arithmetic x&lt;-2 y&lt;-4 y + x y - x y * x y / x Powers y^2 y^3 Roots sqrt(y) 1.9.1.1 PRACTICE PROBLEM 5 Create an object called ‘age’ that takes the value of your age in whole years. Use the ‘age’ object you created in (a) to create a second object called ‘weeks’ that takes the value of your age in whole weeks (assume 52 weeks in each year and disregard partial years). # SOLUTION age &lt;- 27 #This is an example, fill in your own age weeks &lt;- age*52 1.9.2 Logical Comparisons y &lt;- 5 x &lt;- 7 w &lt;- 5 Check equality y == x y == w Check relative size y &gt; x # greater than y &gt;= x # greater than or equal to y &lt; x # less than y &lt;= x # less than or equal to y &gt; w y &gt;= w y &lt; w y &lt;= w We can negate any logical condition by prepending the ‘!’ character y &gt; x !y &gt; x y == w y != w We can create more complex logical conditions with the AND and OR operators: ‘&amp;’ and ‘|’ y == w &amp; y &lt; x y == w &amp; y &gt; x y == w | y &gt; x 1.9.2.1 PRACTICE PROBLEM 6 Use a single line of code to generate a logical value (i.e., TRUE/FALSE) indicating if the value of the weeks object you created above is larger than the age object you created. # SOLUTION weeks &gt; age 1.10 Data structures In SPSS or Stata, data is organized in a rectangular data frame, with cells arranged in rows and columns. Typically, the rows then represent cases (e.g., respondents, participants, countries) and columns represent variables (e.g., age, gender, education level, GDP). For most analyses, this is also the recommended data format in R, using the data.frame structure. However, an important difference is that in R it is possible, and often useful, to combine different formats. Also, to understand how a data.frame in R works, it is useful to understand that a data.frame is a collection of vectors, and thus it is useful to first understand how vectors work. Here we will first briefly discuss vectors, and then quickly move on to data.frames. In addition, there are other common data structures, such as the matrix and list, which we will not discuss here. Different packages can also provide new classes for organizing and manipulating data. 1.10.1 Vectors The concept of a vector might be confusing from a social science background, because we rarely use the term in the context of statistics (well, not consciously at least). We won’t address why R calls them vectors and how this relates to vector algebra, but only how you most commonly use them. A vector in R is a sequence of one or more values of the same data type. From a social science background, it is very similar to what we often call a variable. You can declare a vector in R with c(...), where between the parentheses you enter the elements, separated with commas. The number of elements is called the length of the vector. A vector can have any of the data types discussed above (numeric, character, factor, logical, Date). v1 &lt;- c(1, 2, 10, 15) ## a numeric vector of length 4 v2 &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;b&quot;) ## a character vector of length 3 v3 &lt;- 1:10 ## a numeric vector of length 10 with the values 1 to 10 If you combine data types in the same vector, R will generally use the broadest data type for the entire vector. For example, if we combine both types in a vector, R will convert the numerical values to character values. c(1, 2, &quot;c&quot;) ## becomes a character vector of length 3 1.10.1.1 Selecting elements There are two common ways to select a specific element or a range of elements from a vector. One is to give the indices (positions) of the elements in square brackets after the vector name. Note that the indices themselves are given as a numeric vector. x &lt;- c(&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;,&#39;e&#39;,&#39;f&#39;,&#39;g&#39;) x[5] ## select the fifth element x[c(1,3)] ## select the first and third elements x[2:5] ## select elements two to five If you select with indices, the specific order of the indices is used, and you can also repeat indices. This can for instance be used to sort data. x[5:1] ## select elements in positions 5 to 1 x[c(5,5,5)] ## select the element in position 5 multiple times You can also use negative indices to select everything except the specified elements. x[-5] ## select every element except the fifth x[-c(1,3)] ## select every element other than the first and third The second way to select values is to use a logical vector, which we don’t cover here. 1.10.2 Data frames A data frame is essentially a collection of vectors with the same length, tied together as columns. This is equivalent to a data matrix that you would use in a program such as SPSS. To create the data frame, we use data.frame(). We enter the vectors in the format: column_name = vector. Here we create a data.frame* for data from a fictional experiment. d &lt;- data.frame(id = 1:10, condition = c(&#39;E&#39;, &#39;E&#39;, &#39;C&#39;, &#39;C&#39;, &#39;C&#39;, &#39;E&#39;, &#39;E&#39;, &#39;E&#39;, &#39;C&#39;, &#39;C&#39;), gender = c(&#39;M&#39;, &#39;M&#39;, &#39;F&#39;, &#39;M&#39;, &#39;F&#39;, &#39;F&#39;, &#39;F&#39;, &#39;M&#39;, &#39;M&#39;, &#39;F&#39;), age = c( 17, 19, 22, 18, 16, 21, 18, 17, 26, 18), score_t1 = c(8.0, 6.0, 7.5, 6.8, 8.0, 6.4, 6.0, 3.2, 7.3, 6.8), score_t2 = c(8.3, 6.4, 7.7, 6.3, 7.5, 6.4, 6.2, 3.6, 7.0, 6.5)) d Now, the data structure clearly implies that there is a relation between the elements in the column vectors. In other words, that each row represents a case. In our example, these cases are participants, and the columns represent: the participant id. the experimental condition (E = experimental condition, C = control group) demographic variables: gender and age. test scores before and after the experimental condition: score_t1 and score_t2 1.10.3 Selecting rows, columns and elements in data frames Since data.frames have both rows and columns, we need to use both to select data. Similar to selection in vectors, we use the square brackets. The difference is that for data.frames the square brackets have two parts, separated by a comma. Assuming our data.frame is called d, we can select with: syntax meaning d[i,j] select rows (i) and columns (j) d[i, ] select only rows (i) and use all columns d[ ,j] select only columns (j) and use all rows Selection for rows (i) and columns (j) works identical to selection in vectors. You can use either a numeric vector with indices, or a logical vector. Accordingly, you can also use comparisons. In addition, there are two special ways to select columns. One is that j can be a character vector with column names. The other uses the dollar sign ($). syntax meaning d[ ,c(“c1”, “c2”)] select the columns with the names “c1” and “c2” d$id select the column named id 1.10.3.1 Selecting columns in data frames Let’s put this to practice, starting with columns: ## selecting a single column returns a vector d[,1] ## select the first column by index d[,&quot;id&quot;] ## select the id column by name d$id ## select the id column using the dollar sign ## selecting multiple columns returns a data.frame d[,1:2] ## select the first two columns by indices d[,c(&quot;id&quot;,&quot;age&quot;)] ## select the &quot;id&quot; and &quot;age&quot; columns by name d[,-1] ## select every column except for the first 1.10.3.2 Selecting rows in data frames Selecting rows is practically identical to selecting elements from vectors, and it conveniently returns a data.frame with all columns and their matched positions intact. d[1:5,] ## select first 5 rows A very useful additional trick is that you can use all the columns to make comparisons. For example, we can use the gender column to look up all elements for which the value is “M” (male), and use this to select rows. d[d$gender == &quot;M&quot;, ] You can combine this with the logical operators to make a selection using multiple columns. d[d$gender == &quot;F&quot; &amp; d$age == 21, ] ## 21 year old female participant(s) d[d$score_t1 &lt; d$score_t2,] ## participants that scored higher after the condition 1.10.3.3 Selecting rows and columns in data frames We can combine row and column selection. This works just like you’d expect it to, so there’s little to add here. Do note, however, that you can combine the different selection methods. d[d$gender == &quot;F&quot;, &quot;score_t1&quot;] ## get the score_t1 column for female participants d[d$gender == &quot;F&quot;,]$score_t1 ## identical, but first subset data.frame, then select column d$score_t1[d$gender == &quot;F&quot;] ## identical, but first select column vector, then subset vector 1.10.4 PRACTICE PROBLEM 7 Subsetting, adding and modifying data With the selection techniques you already learned how to create a subset of the data. Try to subset the data so that only participants in the condition “E” are included. Assign this subset to a new name. Create a second subset in which only the variables id, gender, and age are included. Assign this subset to a different name. # SOLUTION d1&lt;- d[d$condition==&#39;E&#39;, ] d2&lt;- d[, c(&quot;id&quot;, &quot;gender&quot;, &quot;age&quot;)] 1.11 Getting data into R There are a number of ways to read data into R. One option is to use the readr package that comes with the tidyverse distribution, read_csv() reads comma delimited files. Download flightdata.csv and store it in your project folder (same folder that you set as your working directory at the beg. of the practical). Most likely, the csv file will open in your browser; just choose “Save as…” (or equivalent) from your browser’s “File” menu so save the file to disk. This file contains a sample from the “flights” dataset from the nycflights13 package. This contains airline data for all flight departing from NYC in 2013. Read the flightdata.csv file into R with the readr package` using the code below # install.packages(&quot;readr&quot;) library(readr) flight_data &lt;- read_csv(&quot;flightdata.csv&quot;) # Imports the data flight_data # View the data To get other types of data into R the tidyverse packages listed below are recommended. haven reads SPSS, Stata, and SAS files readxl reads excel files (.xls and .xlsx) 1.11.1 Summarising the data Again, there are different functions to summarise data, but summary() that is available in base R works well too. Applying summary() to the data summary(flight_data) # `summary()` returns various summary statistics for every numeric variable including minimum and maximum, quartiles, mean and median. Notice that for character type variables, this returns the length of the variable. 1.12 Saving data Of course, you can also save a dataset in our computer in a variety of formats (e.g., .RData, .csv…). When saving, it is always important to understand where you are saving the file. Each R session is connected to a particular folder on your computer. you can check where you are by using getwd() which you were introduced to earlier. This resulting path is where the data will be stored. # Where are we currently on our computer? getwd() # Save and load an RData file save(d, file = &quot;test_data.RData&quot;) ## file = file and/or connection to write to load(file = &quot;test_data.RData&quot;) # Save and load a csv file write_csv(d, file = &quot;test_data.csv&quot;) d2 &lt;- read_csv(file = &quot;test_data.csv&quot;, ) d2 1.13 Functions Where data types and structures concern how data is represented in R, functions are the tools that you use to read, create, manage, manipulate, analyze and visualize data. A function is a set of statements organized to perform a specific task. It is an object that accepts a certain number of arguments and then returns the result of the task performed on these arguments. R has a large number of built-in functions and you can also create your own functions. For now, we will focus on the former. Simple examples of built-in functions are seq(), mean(), min(), max(), and sum(). While there are many correct and formal ways to define what functions are, for the sake of simplicity we will focus on an informal description of how you can think of functions in R: A function is used in the following way: output &lt;- function_name(argument1, argument2, ...) function_name is a name to indicate which function you want to use. It is followed by parentheses. arguments are the input of the function, and are inserted within the parentheses. Arguments can be any R object, such as numbers, strings, vectors and data.frames. Multiple arguments can be given, separated by commas. output is anything that is returned by the function, such as vectors, data.frames or the results of a statistical analysis. Some functions do not have output, but produce a visualization or write data to disk. The purpose of a function is to make it easy to perform a (large) set of (complex) operations. This is crucial, because It makes code easier to understand. You don’t need to see the operations, just the name of the function that performs them. You don’t need to understand the operations, just how to use the function. For example, say that you need to calculate the square root of a number. This is a very common thing to do in statistical analysis, but it actually requires a quite complicated set of operations to perform. This is when you want to use a function, in this case the sqrt (square root) function. sqrt(5) In this example, the function name is sqrt. The input is the single argument 5. If you run this code, it produces the output 2.236068. Currently, R will simply print this output in your Console, but as you learned before, we can assign this output to a name. square_root &lt;- sqrt(5) This simple process of input -&gt; function -&gt; output is essentially how you work with R most of the times. You have data in some form. You provide this data as input to a function, and R generates output. You can assign the output to a name to use it in the next steps, or the output is a table with results or a visualization that you want to interpret. 1.13.1 Using functions Above you saw the simple function sqrt(), that given a single number as input returned a single number as output. Functions can also have multiple arguments as input. Consider this function here: mean(x = c(1, 2, NA, 1, 2, 5), na.rm = TRUE) This function, with the name mean, is given several arguments here: x and na.rm. Given this input, many operations are performed behind the scenes to create the arithmetic mean of the vector. By now we hope you have realized just how broad the use of functions is. The R syntax for performing basic mathematical operations such as sqrt() is essentially the same as the syntax for creating a mean, performing advances statistical analysis or creating powerful visualizations. Accordingly, if you understand this syntax, you can do almost anything in R. 1.13.2 PRACTICE PROBLEM 8 Perform the following operations: 1. Create a sequence of numbers from 12 to 24. HINT: look up ?s eq 2. Sum the numbers from 20 to 60. 3. Find the mean of the numbers from 25 to 80. # SOLUTION seq(from =12, to=24) sum(seq(from=20, to =60)) mean(seq(from=25, to =80)) The many R packages that you can install are mostly just collections of functions. We realize that it is a lot and might be a bit overwhelming. So we will now show you how to use the R help option which allows you to view and interpret any function documentation page and learn how to use it. 1.14 Getting help: viewing and interpreting function documentation You can access the documentation of a function by typing a question mark in front of the function name, and running the line of code. Let’s do this to view the documentation of the sqrt() function ?sqrt Alternatively, you can also use the ‘help’ function help(sqrt) help(&quot;sqrt&quot;) If you run this in RStudio, the help page will pop-up in the bottom-right corner, under the Help tab page. Sometimes, if the name of a documentation page is used in multiple packages, you will first receive a list of these packages from which you will have to select the page. For the sqrt() function, the help page has the title “Miscellaneous Mathematical Functions”. Just below the title, you see the Description, in which the author of a function briefly describes what the function is for. Here we see that there are two functions that are grouped under “Miscellaneous Mathematical Functions”, the abs() function for computing the absolute value of a number x, and the sqrt() function for the square root. Under description, the Usage is shown. This is simply the name of the function or functions, and the possible arguments that you can use. Here the Usage is extremely simple: both functions only take one argument named x. In a minute, we’ll discuss functions with multiple arguments. Below usage, the Arguments section explains how to use each argument. Here, the only argument is x, and it is explained that x is “a numeric or complex vector or array”. For now, let’s focus only on the case of a numeric vector. It appears that in addition to giving a single value like above (recall that in R this is actually a vector of length 1) we can give a vector with multiple numbers. sqrt(c(1,2,3,4,5)) There are more parts to the documentation that we’ll ignore for now. Notable parts to look into for yourself are Details, that provides more information, and the Examples section at the very bottom, which is a great starting point to see a function in action. Note Non-letter characters need to be quoted # ?/ will not work # help(/) will not work ?&quot;/&quot; # will work help(&quot;/&quot;) # will work Note II If a package is not loaded, you need to specify the namespace install.packages(&quot;mice&quot;, repos = &quot;http://cran.us.r-project.org&quot;) # ?quickpred will not work ?mice::quickpred help(quickpred, package = &quot;mice&quot;) Note III You can also open an interactive web-based help page help.start() 1.14.1 PRACTICE PROBLEM 9 Access the help file for the vector() function. How many arguments does the vector() function take? # SOLUTION ?vector #the vector() function takes two arguments, length and mode "],["introduction-to-web-scraping-in-r.html", "2 Introduction to web scraping in R 2.1 Some preliminaries: about websites, scraping, and crawling 2.2 Getting our tools ready 2.3 Getting to know the target website 2.4 Are we allowed to scrape? 2.5 Making a plan 2.6 Locating the correct information 2.7 Building the scraper 2.8 Answering the research question 2.9 Extensions", " 2 Introduction to web scraping in R This is a tutorial to illustrate the use or R for scraping in a social science context. The application is the Dutch website www.petities.nl, which is a platform where anyone can start a petition for any cause and collect signatures. Such websites fit into a larger trend in which citizens increasingly use the power of the internet to challenge the political status quo. The petitions listed here address a large variety of issues, some more serious than others. Interestingly, some petitions become very successful, while other peter out quickly. What drives such processes? To start addressing this question, we collect some data from this website using a scraper that we build in R. As a motivating research question, let’s state the following simple descriptive question: what does the variation in success between petitions look like? For a first hypothesis on what this might look like, have a look at Fig. 3.4 in the chapter by Margetts et al., as discussed in the seminar. 2.1 Some preliminaries: about websites, scraping, and crawling To get started, we need to understand a bit about the technical nature of web scraping. What is web scraping? For our context, which is social science-oriented research, we typically refer to web scraping as the automated collection of information from websites. In principle, there is nothing fancy about collecting or downloading information from websites. In fact, when you view a website, your browser does exactly this: it downloads some file from a server (such as an html-file) and renders it in a way that looks pretty. If you go to www.petities.nl, you can view this file by right-clicking on the page and selecting “view page source” (in various browsers this may be called slightly differently), which will, in this case, show the the HTML-code which produces the page. The information we want to collect for our research is somehow embedded in this code, and to view it we are already downloading it, so downloading the information per se is not so much the challenge. The real challenges in a web scraping project are a little bit more specific, namely: We want to download a large amount of information in a systematic and automated way; We want to process the information in a way such that we can easily analyze the data later. Step 1 above typically involves that we want to automatically visit a number of web pages, in a systematic way, even if we don’t know all the URL’s of these pages beforehand. For example, on a social media website, we may want to get a list of a given user’s followers, and once we have these followers, their followers, etc. This process of discovering websites by following links is called crawling. Web scraping, in the strict sense of the word, refers to the downloading (and processing) of the information found on those websites. A typical data collection task, then, does both: crawling the web to find relevant websites in a specified way, and in the process storing information from these websites. Generally, when we speak about “web scraping” or “a web scraper”, we actually mean a process or a piece of software that does both crawl and scrape. So let’s build a web scraper for petities.nl. 2.2 Getting our tools ready First, we need to collect some tools, starting with some R packages. Start a new R script, copy and paste the following code, and run it: install.packages( c( &quot;rvest&quot;, # the main package for scraping &quot;stringr&quot;, # some useful functions for working with strings &quot;tidyverse&quot;), # for general data handling repos = &quot;http://cran.us.r-project.org&quot; ) We then activate these packages for this session with library commands: library(rvest) #scraping library(stringr) # string functions library(tidyverse) # for general data handling Second, we need to add some special tool to our browser to get a better understanding of -and grip on- the website that we are scraping. I use Firefox, for which the add-on “ScrapeMate” is available. Chrome has different extensions for the same purpose (such as SelectorGadget), and so may other extendable browsers. Note that results may differ between browsers and extensions from website to website, so if you can’t figure out a website (you’ll see what I mean by that later) using one particular browser-extension combination, it’s always worth trying other combinations. Go go ahead and install the relevant extension for your browser. 2.3 Getting to know the target website Before we start coding, it’s always a good idea to get a little bit acquainted with the website we’re scraping. Go to Petities.nl and try to anwer the following questions: Which information is shown about each petition? How are petitions sorted? What different options do we have to show and sort petitions? Which one do you think is most useful for our purpose? What do the numbers in the bottom left and -right of each “petition box” mean? What is your impression about the sort of petitions that are posted here? Does anything stand out? Also, once more look at the page source code (see above). Which part of the content of the page can you already identify in the source code (and which not)? You’ll notice that there is a lot of code that does not seem to refer to any content; things like class=\"search-container whitespace\". This is called CSS (for Cascading Style Sheets) and it determines what a website looks like, by applying certain formatting rules to things that should look familiar (not unlike, for example, the style templates of Microsoft Word or Powerpoint). We’ll take advantage of this later to locate information on the page. 2.4 Are we allowed to scrape? There’s a lot to say about the ethics and legal aspects of web scraping, which we will not cover here; we specifically chose a target website for this tutorial for which these concerns are minimal. For any real research project, a careful ethical review is part of responsible research practices. Generally speaking though, if you’re not collecting personal data, you’re mostly fine from a legal perspective (but don’t take this as legal advice). That does not mean that every website likes to be scraped. For example, they may want to protect their data, or prevent excessive server traffic. Mostly, the scrapers or crawlers they would be concerned about are not in fact those used by researchers, but by search engines like Google. In order to communicate their preferences, websites may use a thing called “robots.txt”, which is a machine- and human-readable file that specifies which parts of the website may be scraped, typically located in the root folder of the website. It is an unwritten rule of the internet to honor the rules specified in robots.txt files (although there may be specific cases where it is ethically justified to ignore them). You can find the robots.txt of petities.nl at https://petities.nl/robots.txt. Click to have a look. The first line, “User-agent:” specifies the types of “agents” to whom the rules that follow apply. “*” here means: everybody. The lines below specify which parts of the websites are allowed or not allowed to scrape. Question:: which parts of the website are disallowed for scraping on petities.nl? Why do you think that is? 2.5 Making a plan Also, before starting to build our scraper (or before any data collection project, really) it is helpful to already have an idea of what we want our final data set to look like. For now, let’s start simple, and aim for a data set that is basically a list of petitions, and which contains for every petition: The title of the petition The url of the petition page The number of signatures As a data frame, the data should eventually look something like this: What the target data set should look like Title link_url Sig_Count “my petition” “https://petities.nl/petitions/my_petition” 9 … … … … … … … … … Check and see whether you can visually identify this information on the website. 2.6 Locating the correct information As a starting piont, let’s take the section of the website that lists all petitions: https://petities.nl/petitions/all?locale=en. When we look at the source code of the page, we can actually already identify all the information that we need in the code. This, however, is rather messy, and not something we can analyze directly. We somehow need to get the information in the shape of our data table above, and for this we first need to automatically extract the titles, URLs, and counts from the messy source code. This is where our browser extension comes in handy. When we look at the website (not the code), we can actually easily identify the relevant information because it is formatted in a consistent way. To achieve this, the website uses CSS tags in the code, and we can take advantage of that to find the relevant information. We could already do that by looking hard at the code, but our browser extension make it easier. Let’s start with the titles. In Firefox: Go to https://petities.nl/petitions/all?locale=en Click the icon for the “Scrapemate” extension in the toolbar (the little wand) In the resulting sidebar on the right, click the top orange button (“start picker”) Click the title of a petition. A few things will happen: All the titles of the petitions shown on the page will be highlighted. This is an indication that you’ve identified the right “field”; we want all those titles! In the bottom right, you will see a list of all the titles. Again, this is what we want! In the bar below the orange button, you’ll see “.petition-overview-info-title”. This is the relevant CSS tag: basically it tells the browser “show this piece of text as a petition title” (and the actual style of a petition title is stored somewhere in a CSS style file, but we don’t care about that). In building our scraper, we can now make use of that: we can basically tell it to “collect all the pieces of text that are formatted like petition titles”, and we now also know that these pieces of text are marked in the source code as “.petition-overview-info-title” (you can actually see this tag in the source code). So let’s get coding! 2.7 Building the scraper 2.7.1 First steps We now turn to the rvest package. To begin, we simply download the entire page: webpage &lt;- read_html(&quot;https://petities.nl/petitions/all?locale=en&quot;) webpage This basically just behaves like a browser: it downloads the source code of the page. It still looks like unintelligible code soup. However, we now know what to look for in this soup. Using the html_nodes() function from rvest, we can identify all the titles: title &lt;- html_nodes(webpage, &quot;.petition-overview-info-title&quot;) title This already looks more structured: it is a list of all the parts in the code that where tagged as .petition-overview-info-title. In the next step, we parse (clean up) this list further to keep only the clean text: title &lt;- html_text(title) title This basically constitutes our variable “title” for our intended data frame, filled with the values of the petitions on this page. Question: how many titles do we have now? And is this the number you would expect? We can now do the same for our next variable, the url. Using our scrapemate tool again (we can either reset our earlier picker by clicking the circle button next to it, or use one of the other pickers in the sidebar), we can determine that the CSS tag in this case is .petition-overview-image-container. The code to get all the URLs then looks like this: link_url &lt;- html_elements(webpage, &quot;.petition-overview-image-container&quot;) link_url &lt;- html_attr(link_url, &quot;href&quot;) Exercise: now write the code to get the number of signatures per petition. 2.7.2 Automating the process: getting multiple pages At this point, we can scrape the required information from the one page, namely, https://petities.nl/petitions/all. However, for our research project, we’re probably interested in more than just these 12 petitions; we want to get a sizable sample of petitions and perhaps even all petitions listed on the website. At the bottom of the page, we see that the list of petitions continues on page 2, 3, etc. Question: how many pages does the list of petitions contain in total? Now of course we could just manually go to the next page, check the url for that page, and repeat our earlier scraping steps to get the petitions for that page. Question: what is the URL of page 2? And page 3? However, this would be very tedious and take a long time. Rather, we’d like to automate this process so that our scraper automatically visits all the pages, and downloads the data. With that, we’re getting to the “crawling” part of web scraping, and it will require a little programming. While this may sound intimidating, it simply means that we are going to “recycle” our earlier instructions to the computer in a smart way. We’ve so far written our code for a specific URL; let’s now write in a way that can be applied to any URL. To do so, we include our earlier code in a function called get_petitions_list(): get_petitions_list &lt;- function(page_url){ webpage &lt;- read_html(page_url) title &lt;- html_nodes(webpage, &quot;.petition-overview-info-title&quot;) title &lt;- html_text(title) link_url &lt;- html_elements(webpage, &quot;.petition-overview-image-container&quot;) link_url &lt;- html_attr(link_url, &quot;href&quot;) sig_count &lt;- html_elements(webpage, &quot;.petitions-counter&quot;) sig_count &lt;- html_text(sig_count) petitions_list &lt;- data.frame(title, link_url, sig_count) return(petitions_list) } If we run this code, nothing really happens yet: all it does is define the function. That is, we can now refer to this set of scraping instructions using the function get_petitions_list(), filling in the URL of the page that we want to scrape as “page_url”. All the code inside the function will then be applied to this page. Specifically, it does the following: Download the source code of the page Extract the titles of the petitions Extract the link urls for each petition Extract the signature counts for each petition Combine the results in a single data frame The last line in the function starting with return defines the result of the function, in this case, the data frame. Let’s test this function on page 2: p2_petitions &lt;- get_petitions_list(&quot;https://petities.nl/petitions/all?locale=en&amp;page=2&quot;) Question: did it work correctly? Now that we have a function to get all the data that we want from a given page in one go, all we need to to is apply this to all the pages we want to scrape and combine the results into a single data frame. To do so, we’ll use a loop: petitions &lt;- get_petitions_list(&quot;https://petities.nl/petitions/all?locale=en&amp;page=1&quot;) # We start with the first page for(i in 2:5){ # Loop throup all values from 2 to 5. The current value is &quot;i&quot; target_page &lt;- paste(&quot;https://petities.nl/petitions/all?locale=en&amp;page=&quot;,i,sep = &quot;&quot;) # Create a string in with we add the current value i to the &quot;stub&quot; of the page url p &lt;-get_petitions_list(target_page) # scrape page i petitions &lt;- rbind(petitions, p) # add the petitions of page i to what we already had } See if you understand the above code with the help of the comments in the code. If you don’t know the additional functions that we use in the loop (for example, paste() or rbind() ), look them up to see what they do. Then, run the code. Question: What do you expect as the result, and is it correct? Question: what would we need to change in the above code to collect data on all petitions on the website? NOTE: at this point, do not actually run the code to collect data for all petitions. While this is tempting, it would 1) take a long time and 2) put an unnecessarily large burden on the Petities.nl servers. In principle, we now have a completely functional scraper! It is capable of automatically visiting a number of pages that we define, and collect the information from those pages that we wanted. Let’s just implement a few small improvements. First, you might have notices that the number of signatures is included in the data frame as a string variable, while it is actually a number. To avoid that we have to fix this afterwords, we can already fix it in our function (see “# NEW”): get_petitions_list &lt;- function(page_url){ webpage &lt;- read_html(page_url) title &lt;- html_nodes(webpage, &quot;.petition-overview-info-title&quot;) title &lt;- html_text(title) link_url &lt;- html_elements(webpage, &quot;.petition-overview-image-container&quot;) link_url &lt;- html_attr(link_url, &quot;href&quot;) sig_count &lt;- html_elements(webpage, &quot;.petitions-counter&quot;) sig_count &lt;- html_text(sig_count) sig_count &lt;- str_replace_all(sig_count, &quot;\\\\.&quot;,&quot;&quot;) # NEW: remove Dutch 1000 separator sig_count &lt;- as.numeric(sig_count) # NEW: change the type from string to numeric petitions_list &lt;- data.frame(title, link_url, sig_count) return(petitions_list) } Second, you may have noticed that scraping five pages (probably) already took a noticeable amount of time. If we’d want to scrape many more pages, we may want to be able to keep track of the progress. For that purpose, we let R print some text to the console to report what going on, in our loop: petitions &lt;- get_petitions_list(&quot;https://petities.nl/petitions/all?locale=en&amp;page=1&quot;) # We start with the first page for(i in 2:5){ # Loop throup all values from 2 to 5. The current value is &quot;i&quot; message(paste(&quot;scraping page&quot;,i,sep=)) #NEW: print what&#39;s happening to the console target_page &lt;- paste(&quot;https://petities.nl/petitions/all?locale=en&amp;page=&quot;,i,sep = &quot;&quot;) # Create a string in with we add the current value i to the &quot;stub&quot; of the page url p &lt;-get_petitions_list(target_page) # scrape page i petitions &lt;- rbind(petitions, p) # add the petitions of page i to what we already had } You may find that you every now and then get a warning “NAs introduced by coercion” (and if not now, you will certainly later). “NA” is R’s term for missing values. If we look at the data (click “petitions” in the environment tab in the top right), we indeed see that some petitions get the value “NA” (that is, missing) for sig_count. Question: What is the issue with these specific petitions (hint: look them up on the website)? And to what extent is this really a problem? 2.8 Answering the research question 2.8.1 Collecting the data Now that we have our scraper ready, we can start to use it to answer our descriptive research question (see above).First we need to collect a larger amount of data. In a real research project, you would probably want to collect the data of all the petitions on the website. However, since this is an educational project, we don’t want to put too much unnecessary strain on the server, and we have many students accessing the server at the same time, let’s limit our data collection to 25 pages. petitions &lt;- get_petitions_list(&quot;https://petities.nl/petitions/all?locale=en&amp;page=1&quot;) # We start with the first page for(i in 2:25){ # Loop throup all values from 2 to 5. The current value is &quot;i&quot; message(paste(&quot;scraping page&quot;,i,sep=)) #NEW: print what&#39;s happening to the console target_page &lt;- paste(&quot;https://petities.nl/petitions/all?locale=en&amp;page=&quot;,i,sep = &quot;&quot;) # Create a string in with we add the current value i to the &quot;stub&quot; of the page url p &lt;-get_petitions_list(target_page) # scrape page i petitions &lt;- rbind(petitions, p) # add the petitions of page i to what we already had } Question: Of how many petitions have you now collected data? 2.8.2 Analyzing the data Next, let’s analyze these data a bit. First, let’s get rid of these annoying “NA” values and replace them by 0. The “coalesce()” function helps with that: petitions$sig_count &lt;- coalesce(petitions$sig_count, 0) # replace missings by 0 For a first look at the data, we can draw a simple histogram: hist(petitions$sig_count) Question: What is your first conclusion about the distribution of success? In their analysis of the data, Margetts et al. visualize the data somewhat differently, in a way that helps to assess the skewness of the distribution a bit better. In particular, they plot the number of signatures against the rank of each petition in terms of signatures, and furthermore, they use logarithmic axes. To recreate their plot, we can do the following: # First we create a rank variable (more signatures = higher rank) petitions &lt;- mutate(petitions, rank = dense_rank(desc(sig_count))) # To avoid that petitions with zero signatures are left out of the plot (because the logarithm of zero is not defined) we add 1 to all values: petitions$sig_count &lt;- petitions$sig_count + 1 # Finally, we plot the data: plot(petitions$rank, petitions$sig_count, log = &quot;xy&quot;) Question:: Compare your figure to Figure 3.4 in the Chapter by Margetts et al. Does the distribution of success in the Netherlands look more like the one in the UK or the one in the US? 2.9 Extensions The way we’ve used rvest now to scrape a website is just scraping the surface (pun intended) of web scraping. Obvioulsy, the internet is vast and diverse, and different circumstances may require different solutions. Here, we briefly discuss a few relevant extensions. 2.9.1 More ethically robust web scraping with polite We’ve not discussed the ethics of web scraping in detail now, but if you collect free data from a website, you should consider what the people behind this website (and behind every website there are people) may think of it. The R package polite takes some of the work involved with that out of your hands. In particular, it: - Automatically check the content of the “robots.txt” file of a website to verify scraping permissions - Let’s you systematically announce you identity to the website’s server. For many cases, this is a useful “automatic” ethical check: if you’re not comfortable announcing your identity while collecting data, that implies that you need to review your research ethics extremely carefully. See here for an online tutorial on using polite. Under the hood, polite still uses rvest for scraping, so it works for every website for which revest works and the basic workflow is the same. 2.9.2 Scraping dynamic websites with RSelenium Rvest works well, but mostly for static websites, that is, websites that are simply stored on a web server and presented to the user “as-is”. It works less well for dynamic websites, which create pages “on the fly” using things like JavaScript. The UU website is an example of this: you won’t be able to easily identify pieces of information on the site using your ScrapeMate (or similar) tool (try it!). For such cases, packages like RSelenium are a solution. In a nutshell, this is a method to scrape websites by mimicking the clicking behavior of a viewer. We do not cover it here as it is much more technically involved and finicky to get working. If your are interested, see here for one of the many online tutorials. "],["collecting-data-through-apis-the-case-of-reddit.html", "3 Collecting data through APIs: the case of Reddit 3.1 Introduction 3.2 Loading the tools 3.3 Case study: Reddit 3.4 Accessing data via the API 3.5 Getting the data with RedditExtractoR 3.6 Creating a network 3.7 Analyzing content: simple sentiment analysis", " 3 Collecting data through APIs: the case of Reddit 3.1 Introduction In this tutorial, we will look at collecting data about human behavior using API access. As an example, we will collect some data from Reddit, the popular online discussion platform. API stands for “application programming interface”, and in general, it is a way in which software applications communicate with each other (instead of with a human user). In the context of websites, web APIs provide means for software applications to access data on web servers. Consider, for example, a smartphone app that allows you to read and post messages on your profile on a social media website such as Facebook. To be able to do so, such an app must be able to communicate with the Facebook server, access information on the server, and send information to the server. For this purpose, Facebook provides an API so that third-party apps can interact with their data. Similarly, a news website that displays the latest “X” (formerly Twitter) trends obtains that information via the X API. In a way, an API is an official “back door” to an application or website designed for other applications or websites; you may compare it with a restaurant that has a front door for guests (in the case of a website: for users) and back door specifically for delivery and employees. In many cases, APIs may also be used for research as they allow researchers to access data stored on web server of interest, such as the web servers of social media services. Web scraping may also be used to access similar data, but there are a number of important differences: As opposed to web scraping, API usage is strictly regulated by the application or website that offers it. That is, the provider of the API determines who can access the API, for what purposes, and under what conditions. Often (but not always), an API requires API users to authenticate in one way or another and to agree to terms and conditions. Sometimes this prevents researchers from using data freely, even if the data are in principle accessible through the API. Nevertheless, as long as you stick to these terms and conditions, getting data from an API implies that you are using the data with permission of the provider, which is not always the case with scraped data. This, however, does not mean that there are never ethical concerns with the use of API data! After all, the individuals who’s data you collect (e.g., social media users) may not agree with your use of the data. This is not different from scraped data. As opposed to scraped data, API-provided data are much more structured, precisely because they are meant to be used. More on that later. A web scraper is typically custom-build by a researcher to collect data from a specific website. In contrast, the technical procedures required to collect API data are much more determined by the provider of the data, allowing for less control by the researcher. 3.2 Loading the tools As before, we’re using a few specific packages for this task. Start a new R-script and copy-paste the below lines to load these packages. In this case, we haven’t included the install.packages() command; since you may already have installed some of these packages before, it’s not efficient to re-install them every time. Thus, if you get a warning like: There is no package called ... you need to install that package first using install.packages() like before (we don’t include that code here). Then, we load the packages: rm(list=ls()) # Start with a clean workspace library(RedditExtractoR) library(tidyverse) library(igraph) library(sentimentr) 3.3 Case study: Reddit Many websites have APIs, and some of them have been extensively used for research, in particular the Twitter API. However, since spring 2023, Twitter (X) no longer provides free access to their API. One of the platforms that still provides access is Reddit, although Reddit also implemented some controversial changes recently . If you’re not familiar with Reddit, please visit the website and browse around a bit (try to find your favorite topics) to make yourself familiar with how it works. In particular, pay attention to: What is a “subreddit”? What is the main purpose (or purposes) of the platform? What are the main properties of “posts”? What are the different ways in which users can interact with the platform (i.e., what are the actions available to users)? 3.4 Accessing data via the API To get data from the API, we can use specific URLs that provide us with the data we want. Instead of a readable webpages, these URLs provide data based on what we specify. For example, to download a list of threads, we could specify the following: https://www.reddit.com/r/{subreddit}/{listing}.json?limit={count}&amp;t={timeframe} Where: {subreddit} The name of the subreddit we want to access; {listing} Determines the order of the list: “controversial”, “best”, “hot”, “new”, “random”, “rising”, or “top”; {limit} The number of desired results; {timeframe} The time frame to which {listing} applies: “hour”, “day”, “week”, “month”, “year”, “all” (i.e., the top posts of the past month). In other words, the URL above says “please give me {limit} {listing} threads from the past {timeframe} from subreddit {subreddit}. Question: consider https://www.reddit.com/r/climate/new.json?limit=1&amp;t=all. What are we requesting here (i.e., what are {subreddit}, {listing}, {limit} and {timeframe}? Question: Now go to https://www.reddit.com/r/climate. Which information here corresponds with what we requested in the previous question? Now click on https://www.reddit.com/r/climate/new.json?limit=1&amp;t=all. What you see here is indeed not a nicely formatted website, but a whole lot of data. The data are structured using the JSON format, which is a common format for exchanging data online. This is a key difference between using an API and typical web scraping: in the latter case, the data are somwhere on a website that is designed to be human-readable, and we have to somehow filter out the relevant information; via the API data are already provided in a nicely structured way, and are intended to be used. Question: Identify the the following information in the JSON file you see in your browser: the title of the post, the name of its author, the current number of comments, and the date/time of posting. We could easily download the JSON data into R using a simple command like this (copy and paste into your R script and run it; ignore the warning): x &lt;- readLines(&quot;https://www.reddit.com/r/climate/new.json?limit=1&amp;t=all&quot;) Subsequently we can run x to view the data (try it), but the result is still quite messy. We could parse the data using R’s built-in JSON tools, but fortunately there is also an R-package specifically for Reddit that makes getting data from the Reddit API into R much more user-friendly. It’s called RedditExtractoR and we’ve already loaded it above. 3.5 Getting the data with RedditExtractoR 3.5.1 Getting threads As a first step, we download all the threads in the Subreddit. All? Not all. It seems actually unclear how many one can download. Choosing “new” and “all” seems to give a relatively large (and sensible) result. threads &lt;- find_thread_urls(subreddit=&quot;climate&quot;, sort_by=&quot;new&quot;, period = &quot;all&quot;) Take a look at the resulting data frame (click on “threads” in the environment tab in the top right corner. Question: How many threads have we downloaded? Which variables are available about each thread? Which thread is the thread that received most comments in these data? Question: Make a histogram of the number of comments. (Hint: look up the code for the histogram that we made in the previous tutorial.) 3.5.2 Getting contents of threads In the next step, we download the contents of these threads. Let’s take only a randomly chosen 50 for simplicity. Note that it is normal that the below code takes a bit of time to run. thread_contents &lt;- threads$url # start with the urls from the threads data frame thread_contents &lt;- sample(thread_contents, 50) # randomly sample 50 thread_contents &lt;- get_thread_content(thread_contents) The resulting object contains two data frames: “threads” and “comments”. The first contains data on the thread as a whole (such as the url, who started it and when, the number of comments, the content of the original post, etc. ). The second contains data of all the comments. Some important variables in “comments” are: url the url of the thread that the comment belongs to. This matches the urls in the “threads” data frame in “thread_contents”. author The author of the comment comment_id The position of the comment in the “tree” of the thread. “1” is the first comment to the original post, “2” is the second, etc. “1_1” is then the first comment to the first comment to the original post, etc. We can take the data frame with comments from the threads_contents object and turn it into its own data frame as follows: comments &lt;- thread_contents$comments Question: How many comments are there in total? Question: Which comment is the most “upvoted” comment? Question: The paper by Treen et al. (2022) used text analysis techniques such as topic modeling in their analysis of polarization on Reddit. Of the data that we have now collected, what do you think they used? 3.6 Creating a network The paper by Treen et al. (2022) aims to assess polarization by, among other things, studying the “reply network” in Subreddits. Question: how do they construct this network, that is, what are the links? And how do they assess the level of polarization? We can partly reproduce their analysis (for “our” Subreddit) using the code below. While this is relatively complicated, see of you can get the gist of what happens. Then, copy-paste and run the code. authors &lt;- select(thread_contents$threads, author, url) # Get the &quot;threads&quot; part of the threat_contents object, keep only the author and url for each thread responders &lt;- select(thread_contents$comments, author, url) # Get the &quot;comments&quot; part of the threat_contents object and keep only the author and url for each thread responders &lt;- rename(responders, responder = author) # rename &quot;author&quot; to &quot;responder&quot; # now match these two together, using the url as the matching variable reply_net &lt;- merge(authors, responders, by = &quot;url&quot;) reply_net &lt;- select(reply_net, author, responder) # keep only the author and responder vars reply_net &lt;- graph_from_data_frame(reply_net) # turn this into a &quot;network object&quot;: something the igraph package for network analysis can work with # plot the network plot(reply_net, vertex.label=NA, vertex.color = &quot;blue&quot;, vertex.size = 5, edge.arrow.size = 0.2, edge.color = &quot;black&quot;, graph.frame = TRUE, main = &quot;The reply network of 50 random threads on r/climate&quot; ) Note that Treen et al.’s network analysis is somewhat more elaborate; for instance, they use a technique called community detection to highlight different subgroups in the network, which we don’t do here for simplicity. Nevertheless, we can still try to assess the polarization of the network loosely by looking at the structure. Question: what would you conclude about polarization in this Subreddit? 3.7 Analyzing content: simple sentiment analysis Besides the structure of the network, we can also analyze the content of the comments. One (relatively) simple way to do this is to perform a sentiment analysis. This is a technique that tries to determine whether a piece of text is positive, negative, or neutral, based on a “dictionary” which assigns positive, neutral or negative sentiments to each work. We can use the `sentimentr’ package for this (already loaded above). One question that we could address is how average sentiments in the comments vary over time. The code below does this for the comments in the data that we collected: # create a data frame of comments from thread_contents$comments comments &lt;- thread_contents$comments # Do the sentiment analysis, storing the results in the object &quot;sentiments&quot; sentiments &lt;- sentiment_by(comments$comment) # Add the average sentiment of each comment to the comments data frame comments$sentiment &lt;- sentiments$ave_sentiment # plot the average sentiment per day comments %&gt;% group_by(date) %&gt;% summarise(ave_sentiment = mean(sentiment)) %&gt;% ggplot(aes(x = date, y = ave_sentiment, group = 1)) + geom_point() + geom_line() + labs(title = &quot;Average sentiment of comments per day&quot;, x = &quot;Time&quot;, y = &quot;Sentiment&quot;) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Question: Now pick another Subreddit on a topic that you find interesting, and try to make a network graph and do a simple sentiment analysis for this Subreddit as well. (Hint: you can reuse most of the code we’ve used above.) "],["a-first-look-at-data-collection-via-the-bluesky-api.html", "4 A first look at data collection via the Bluesky API 4.1 Introduction 4.2 Prerequisites 4.3 Loading the packages 4.4 Getting started: authenticating 4.5 Getting data 4.6 Working with the data", " 4 A first look at data collection via the Bluesky API 4.1 Introduction Recently, the social media platform Bluesky has become increasingly popular. Bluesky is a “microblogging” platform that is, in many ways, comparable to Twitter (X). Bluesky also has an API that allows to access some of its datat. In this tutorial, we will explore some of the possibilities of doing this in R. At the moment, it is still rudimentary, and it will hopefully be expanded in the future. 4.2 Prerequisites To start with, you’ll need a Bluesky account. You can sign up at https://bsky.app/. Once you have an account, you need to create an “app password”, which you will use in your code to get access to the API. To create an app password, go to your Bluesky home page and click Settings&gt;Privacy and Security&gt;App Passwords&gt;Add App Password. In the next screen, choose an arbitrary name for your app password (e.g., “SRDA”). The click Next, which will show you your app password. Copy this password and paste it somewehere, as you will not be able to see it again. We will use this password in the code below. WARNING: while the app password does not give full access to your account, it can be used to post messages on your behalf. So, keep it safe and do not share any code that contains your app password publicly. If you fear that your app password has been compromised, or you simply don’t need it anymore, you can always delete it in your Bluesky settings. I recommend doing this after you have finished this tutorial. 4.3 Loading the packages To access the Bluesky API, we will use the bskyr package. We will also use the tidyverse package to work with the data. If you don’t have these packages installed, install them first. We load the packages with the following code: library(bskyr) library(tidyverse) 4.4 Getting started: authenticating First, we need to authenticate with the Bluesky API, using your username and your app password: set_bluesky_user(&quot;[yourusername].bsky.social&quot;) set_bluesky_pass(&quot;[your app password]&quot;) auth &lt;- bs_auth(user = bs_get_user(), pass = bs_get_pass()) # This will authenticate you with the Bluesky API and store the authentification in the object &quot;auth&quot; 4.5 Getting data Now that we are authenticated, we can start getting data from Bluesky. The bskyr package has several functions that allow you to get data from Bluesky. For example, you can get the followers of a user, the profile of a user, or search for posts. Below are some examples of how to use these functions: followers &lt;- bs_get_followers(actor = &#39;chriskenny.bsky.social&#39;, auth = auth) #Chris Kenny is the author of the package profile &lt;- bs_get_profile(actor = &#39;chriskenny.bsky.social&#39;, auth = auth) # Get Chris&#39; profile information posts &lt;- bs_search_posts(query = &quot;bezuinigingen&quot;, auth = auth) # Search for posts that contain the word &quot;bezuinigingen&quot; Explore the data objects that you get from these functions, and also have a look at Chris Kenny’s actual profile page. You will note that the numbers that you get through the API are sometimes lower than what you see “in the wild”: this is because by default, the API only returns a limited number of results. You can typically change this number by setting the “limit” parameter in the respective functions. For example: followers &lt;- bs_get_followers(actor = &#39;chriskenny.bsky.social&#39;, limit = 200, auth = auth) #Chris Kenny is the author of the package 4.6 Working with the data When we look at the “posts” data object, we find that it is quite complicated: it is a list of lists of lists. Extracting the right data from this requires some work. For example, we can get the text of each post as follows: texts &lt;- sapply(posts$record, function(x) x$text) # extracting the data from the complex object texts &lt;- as_tibble(texts) # turn it into a neat data frame texts &lt;- rename(texts, text = value) # rename the column Next, we can get all the authors of these posts: author_handle &lt;- sapply(posts$author, function(x) x$handle) author_handle &lt;- as_tibble(author_handle) author_handle &lt;- rename(author_handle, author_handle = value) Finally, we can combine these two data frames into one: posts_df &lt;- cbind(texts, author_handle) We could use this data frame for subsequent analysis, such as sentiment analysis or topic modeling.There are many more functions in the bskyr package that you can use to get data from Bluesky. You can find more information in the package documentation. "],["analyzing-experimental-data.html", "5 Analyzing experimental data 5.1 Introduction 5.2 Replicating the results", " 5 Analyzing experimental data 5.1 Introduction Last week, you participated in a mock-up version of Stein et al.’s (2023) experiment about the propagation of misinformation in social (media) networks. This week, we’ll have a quick look at the results to see whether they more or less conform to what Stein et al. found. For reference, have a look at the figure with their main results. They key interest there is in the spread of false messages that align with the participant’s ideology, and the key comparison is comparison is between “integrated” and “segregated” networks. Put very simply, this figure shows that while there is no difference in sharing behavior as such between the two types of networks (left), subjects are much more likely to receive messages that align with their ideology in the segregated network than in the integrated network. 5.2 Replicating the results Download exp_clean_UU.dta. This dataset contains the cleaned microdata of the experiment from last Monday. Note that this is a Stata datafile. In this dataset, rows represent subjects’ decisions to share or to reject a message. The variable ‘sbj_share’ tracks if someone shared (1) or not (0). The data contains subject (sbj) properties, such as an anonymized subject ID and whether the subject is a liberal (lib). Variables with msg in the beginning track message properties. Variables with nw indicate network properties. Load the dataset (R can load Stata files with the haven package) and have a look at the data to see whether you understand the structure of the dataset and the variables. Generate a dummy (0 or 1) variable ‘align’ that indicates whether subject and message have the same ideology. Next, we replicate the result from Figure 3A in Stein et al. That is: what is the mean probability of being shared of false aligned messages, for each of the two network types? Note that: The probability of a 0-1 variable to be 1 is simply the mean of this variable. We’re only interested in cases where the message is false and aligns with the participant’s ideology. We need to calculate the mean probability of being shared per message, because we have many decisions per message. For this, we use the group_by() and summarise() functions . Then, you can compare the mean of this in turn between the two networks. # Load necessary libraries library(haven) # For loading Stata .dta files library(dplyr) # For group manipulation and summarisation # Load the dataset data &lt;- read_dta(&quot;exp_clean_UU.dta&quot;) # Generate the &#39;align&#39; variable data$align &lt;- data$sbj_lib == data$msg_lib # First collapse data_collapsed1 &lt;- group_by(data, nw_segregated, msg_id, align, msg_true) data_collapsed1 &lt;- summarise(data_collapsed1, sbj_share = mean(sbj_share, na.rm = TRUE), .groups = &quot;drop&quot;) # Second collapse data_collapsed2 &lt;- group_by(data_collapsed1, nw_segregated, align, msg_true) data_collapsed2 &lt;- summarise(data_collapsed2, sbj_share = mean(sbj_share, na.rm = TRUE), .groups = &quot;drop&quot;) data_collapsed2 &lt;- filter(data_collapsed2, align == TRUE &amp; msg_true == 0) # Result data_collapsed2 How do the results compare to the original result by Stein et al.? How do you interpret the similarities and differences? Next, we’ll replicate the result of 3B. That is: how does the mean probability of being exposed to a false message that aligns with your ideology differ between the two network types? We provide you with the R code for this: # Load necessary libraries library(haven) # For loading Stata .dta files library(dplyr) # For data manipulation # Load the dataset data &lt;- read_dta(&quot;exp_clean_UU.dta&quot;) # Generate the &#39;align&#39; variable (if you had not done so already) data$align &lt;- data$sbj_lib == data$msg_lib # Collapse to the number of observations (that is the exposure count) by message ID, type of network, whether the message is true, alignment, and network size. collapsed_data_count &lt;- group_by(data, msg_id, nw_segregated, align, msg_true, nw_size) # Collapse to the median number of observations, by veracity (msg_true), network type &amp; size, and alignment. collapsed_data_count &lt;- summarise(collapsed_data_count, sbj_share = n(), .groups = &quot;drop&quot;) # Collapse (median) sbj_share by specified variables collapsed_data_median &lt;- group_by(collapsed_data_count, nw_segregated, align, msg_true, nw_size) collapsed_data_median &lt;- summarise(collapsed_data_median, sbj_share = median(sbj_share), .groups = &quot;drop&quot;) # Generate &#39;exposure_prob&#39; collapsed_data_median$exposure_prob &lt;- collapsed_data_median$sbj_share / (collapsed_data_median$nw_size / 2) result &lt;- filter(collapsed_data_median, align == TRUE &amp; msg_true == 0) result Again, how do the results compare to the original result by Stein et al.? How do you interpret the similarities and differences? Lastly, we reproduce the result in Figure 4A. Basically, this is about the average percentage of false messages (misinformation) circulating in the population in each network. Again we provide you with the R code: library(reshape2) # For reshaping data # Step 1: Collapse (sum) sbj_share by msg_true, msg_id, nw_segregated collapsed_data_sum &lt;- aggregate(sbj_share ~ msg_true + msg_id + nw_segregated, data = data, FUN = sum) # Step 2: Collapse (median) sbj_share by msg_true, nw_segregated collapsed_data_median &lt;- aggregate(sbj_share ~ msg_true + nw_segregated, data = collapsed_data_sum, FUN = median) # Step 3: Reshape the data to wide format with i(nw_segregated) and j(msg_true) reshaped_data &lt;- dcast(collapsed_data_median, nw_segregated ~ msg_true, value.var = &quot;sbj_share&quot;) colnames(reshaped_data) &lt;- c(&quot;nw_segregated&quot;, &quot;sbj_share0&quot;, &quot;sbj_share1&quot;) # Step 4: Generate fraction_f reshaped_data$fraction_f &lt;- (reshaped_data$sbj_share0 / (reshaped_data$sbj_share0 + reshaped_data$sbj_share1)) * 100 result &lt;- reshaped_data result How do the results compare to the original result by Stein et al.? How do you interpret the similarities and differences? "],["further-resources.html", "6 Further resources 6.1 Collecting data from various online platforms 6.2 Further reading", " 6 Further resources 6.1 Collecting data from various online platforms Here, we discuss methods to collect data from various other social media platforms, with a focus on R packages that facilitate this. Where possible, we illustrate the use of these packages with brief examples. As APIs change often, all of this comes “without warranty” and may be outdated by the time you read this. 6.1.1 Bluesky Bluesky is a relatively new social media platform that originated as a side project of Twitter, and which is indeed very much like Twitter in terms of functionality. Since the turbulent changes at Twitter (now X), Bluesky has become popular as an alternative, in particular among academics. Bluesky is open-source and has a public API, which makes it a good platform for data collection. We can collect data from it in R using the bskyr package. library(bskyr) In order to access the API, you first need to: Create a Bluesky account Create an app password in your account settings. Log into your Bluesky account, go to “Settings”, “Privacy and security”, and then to “App passwords”. Click ” Add App Password”. Give the new App Password a sensible name; since you will this specifically with bskyr, you could for example call it “bskyr” . Click next; the website will create an App Password for you. Copy this password, as you will not be able to see it again in Bluesky! Once you click “Done”, the password will be listed under its name in the App Passwords section of your account; you can always delete it from there if you don’t need it anymore. Any code that uses this password will then no longer be able to use it. We can now use this password to access the API. We create an authentication token “auth” that we can use later in our code: set_bluesky_user(&quot;[your Bluesky handle]&quot;) set_bluesky_pass(&quot;[your app password]&quot;) auth &lt;- bs_auth(user = bs_get_user(), pass = bs_get_pass()) Now, we can start collecting some data from the API. For example, we can get my profile as follows: user &lt;- &#39;rensec.bsky.social&#39; profile &lt;- bs_get_profile(actor = user, auth = auth) The resulting data object contains a lot of data, including the number of followers: followers_count &lt;- as.numeric(profile$followers_count) # Note that the count was originally stored as a string followers_count We can also get the followers themselves. In the function call, we set limit to the previously retrieved number of followers to make sure that the API returns all the followers. followers &lt;- bs_get_followers(actor = user, auth = auth, limit = followers_count) Note that for users with a lot of followers, this may take a few minutes, and it becomes increasingly likely that the process breaks down due to short interruptions in the connection. Nevertheless, in principle it is possible to for example draw a snowball sample, repeating the above steps while looping through the followers of each retrieved user. The package also includes functions for retrieving posts, relationships between users, and many other things; see the documentation. 6.1.2 TikTok The popular video platform TikTok may be studied using the traktok package. It is not (yet?) available at CRAN, so we have to install it from Github, for which you first need to install the package remotes. library(remotes) remotes::install_github(&quot;JBGruber/traktok&quot;) TikTok has two APIs: the “Research API” aimed at researchers in Europe and the US, and the “unofficial” or “hidden” API which is what TikTok uses to function internatlly. Using the Research API requires approval from TikTok, which supposedly may take up to four weeks. In contrast, the unofficial API can be used without approval. More information on usage and research examples can be found at https://jbgruber.github.io/traktok/index.html. 6.2 Further reading Van Atteveldt, W., Trilling, D., &amp; Calderón, C. A. (2022). Computational Analysis of Communication. Wiley Blackwell. –&gt; An open access computational social science textbook giving a practical introduction to the analysis of texts, networks, and images with code examples in Python and R. Somewhat communication science-oriented. Wickham, H., Çetinkaya-Rundel, M., &amp; Grolemund, G. (2023). R for Data Science: Import, Tidy, Transform, Visualize, and Model Data. O’Reilly. –&gt; The meanwhile classic open textbook on using R for data analysis. Focuses mostly on data handling and exploratory analysis; does not cover statistical methods. "],["extra-data-manipulation-and-analysis-with-r.html", "7 Extra: Data manipulation and analysis with R 7.1 Introduction 7.2 Preparation 7.3 Data preparation with base R 7.4 Data manipulation using Tidyverse 7.5 Adding or transforming variables with mutate() 7.6 Working with Pipes", " 7 Extra: Data manipulation and analysis with R 7.1 Introduction Welcome to the second week of Part I - Introduction to R! The aim of this practical is for you to further learn how to (i) manipulate data (particularly using Tidyverse) and (ii) perform basic data analysis in R. You will also be introduced to the lavaan package which we will extensively use during Parts II and III of the course. The contents of this practical are based on materials developed by Kyle Lang (https://github.com/kylelang/Introduction-to-R) as well as Kasper Welbers, Wouter van Atteveldt, and Philipp Masur (https://github.com/ccs-amsterdam/r-course-material). 7.2 Preparation Load necessary packages library(psychTools) # for the example dataset Load the ‘bfi’ data from the ‘psychTools’ package data(bfi, package = &quot;psychTools&quot;) Save the original variable names for later use varNames0 &lt;- colnames(bfi) 7.3 Data preparation with base R 7.3.1 Subsetting We’ve already seen several flavors of subsetting using the ‘[]’, ‘[[]]’, and ‘$’ operators. Since this is very important and is virtually always required before performing any statistical analyses, let’s practice this again. Select the ‘age’ column bfi$age bfi[[&quot;age&quot;]] bfi[ , &quot;age&quot;] bfi[&quot;age&quot;] Select the first 10 rows bfi[1:10, ] Select columns 1, 3, and 5 bfi[ , c(1, 3, 5)] Select the ‘age’ and ‘gender’ columns bfi[ , c(&quot;age&quot;, &quot;gender&quot;)] bfi[c(&quot;age&quot;, &quot;gender&quot;)] 7.3.1.1 Other subsetting methods Now we’ll look into some other methods of subsetting rows and columns of a data frame. Select the first 20 rows of all columns with names beginning in “A” bfi[1:20, grep(&quot;^A&quot;, colnames(bfi))] Select the first 10 rows head(bfi, 10) Select the final 15 rows tail(bfi, 15) Select the first 5 rows and exclude columns 2 through 4 bfi[1:5, -(2:4)] Select rows 42, 45, and 56 and exclude the ‘age’ and ‘education’ columns bfi[c(42, 45, 56), setdiff(colnames(bfi), c(&quot;age&quot;, &quot;education&quot;))] We can also subset using logical vectors. Create a logical vector flagging any participant older than 30 over30 &lt;- bfi$age &gt; 30 sum(over30) # Checking how many people that is mean(over30) # Checking what proportion of the total sample that is We can use this logical vector as a filter by using it to index rows bfi30 &lt;- bfi[over30, ] dim(bfi30) # checking the dimensions of the data frame (number of rows by columns) min(bfi30$age) # checking the minimum value of the age variable We can define our logical filter vector using arbitrarily complex conditions. For example, select males who are younger than 50 and have at least a bachelor degree. filter &lt;- with(bfi, age &lt; 50 &amp; gender == 1 &amp; !is.na(education) &amp; education &gt;= 4) sum(filter) # check how many observations we got bfi2 &lt;- bfi[filter, ] # do the actual selection # Check the results; check whether the filter worked correctly nrow(bfi2) unique(bfi2$gender) max(bfi2$age) min(bfi2$education) 7.3.1.2 PRACTICE PROBLEM 1 Use base R subsetting procedures to select the five neuroticism items for female minors out of the ‘bfi’ data. # SOLUTION 7.3.2 Sorting We can use the sort() function to order the elements of a vector x &lt;- runif(6) # creating the vector (6 random draws from a uniform distribution) sort(x) # sorting in increasing order sort(x, decreasing = TRUE) # sorting in decreasing order Note that, since the draws are random, the values that you got may be different from those above. In order to get the same values each time you run this function (or any other function with a random component), you can use a seed which can be any combination of numbers. set.seed(123) x_2&lt;-runif(6) To sort the rows of a matrix or data frame using base R functions, we can use the order() function x order(x) # Creating a data frame (y &lt;- data.frame(x1 = x, x2 = rnorm(6), # (6 random draws from a normal distribution) x3 = rep(letters[1:2], 3) ) ) y[order(y$x1), ] y[order(y$x1, decreasing = TRUE), ] 7.3.2.1 PRACTICE PROBLEM 2 Use base R functions to sort the ‘bfi’ data on ascending order of ‘age’ # SOLUTION 7.3.3 Transformation One common type of data transformation is converting numeric or character variables into factors. A quick-and-dirty solution uses the as.factor() function to cast the variable to a factor with default labels. (animals &lt;- sample(c(&quot;dog&quot;, &quot;cat&quot;, &quot;mongoose&quot;), 25, TRUE)) # creating the variable (animalsF &lt;- as.factor(animals)) # converting to a factor levels(animalsF) # checking values of the variable table(character = animals, factor = animalsF) # checking if it worked properly While the quick-and-dirty solution works fine for converting character vectors with meaningful values, it isn’t so nice for numeric variables. genderF &lt;- as.factor(bfi$gender) levels(genderF) table(numeric = bfi$gender, factor = genderF) We can use the factor() function to build exactly the factor we want bfi0 &lt;- bfi bfi$gender &lt;- factor(bfi$gender, labels = c(&quot;male&quot;, &quot;female&quot;)) levels(bfi$gender) table(numeric = bfi0$gender, factor = bfi$gender) 7.3.3.1 PRACTICE PROBLEM 3 Convert the ‘education’ variable into a factor and assign it the following levels 1 - ‘no education’, 2 - ‘primary’, 3 - ‘secondary’, 4 - ‘bachelor’, 5 - ‘master and above’. HINT: it is better to create a new education variable (e.g., education2) so then the values of the new variable can be verified with those of the original one. # SOLUTION 7.3.4 Creating new variables A lot of what we’ve learned so far can also be used to create new variables. To create a new variable we need to assign a new column to our data frame (and give it any name we want). bfi$agree&lt;- bfi$A1 + bfi$A2 + bfi$A3 + bfi$A4 + bfi$A5 bfi$agree2&lt;- rowSums(bfi[1:5], na.rm = F) # sum of all agree personality trait components head(bfi$agree) head(bfi$agree2) summary(bfi$agree) summary(bfi$agree2) bfi$agree_mean = rowMeans(bfi[1:5]) bfi$agree_mean2 = rowMeans(bfi[c(&quot;A1&quot;, &quot;A2&quot;, &quot;A3&quot;, &quot;A4&quot;, &quot;A5&quot;)]) # mean of all agree personality trait components summary(bfi$agree_mean) summary(bfi$agree_mean2) 7.4 Data manipulation using Tidyverse The goal of this part of the practical session is to get you acquainted with Tidyverse. Tidyverse is a collection of packages that have been designed around a singular and clearly defined set of principles about what data should look like and how we should work with it. It comes with a nice introduction in the R for Data Science book. This tutorial deals with most of the material in chapter 5 of that book. In this part of the tutorial, we’ll focus on working with data using the tidyverse package. This package includes the dplyr (data-pliers) package, which contains most of the tools we’re using below, but it also contains functions for reading, analysing and visualising data that will be explained later. 7.4.1 Installing tidyverse As before, install.packages() is used to download and install the package (you only need to do this once on your computer) and library() is used to make the functions from this package available for use (required each session that you use the package). install.packages(&#39;tidyverse&#39;) # only needed once library(tidyverse) Note don’t be scared if you see a red message after calling library. RStudio doesn’t see the difference between messages, warnings, and errors, so it displays all three in red. You need to read the message, and it will contain the word ‘error’ if there is an error, such as a misspelled package. library(tidyvers) # this will cause an error! 7.4.2 Tidyverse basics As in most packages, the functionality in dplyr is offered through functions. In general, a function can be seen as a command or instruction to the computer to do something and (generally) return the result. In the tidverse package dplyr, almost all functions primarily operate on datasets, for example for filtering and sorting data. With a data set we mean a rectangular data frame consisting of rows (often items or respondents) and columns (often measurements of or data about these items). These datasets can be R data.frames, but tidyverse has its own version of data frames called tibble, which is functionally (almost) equivalent to a data frame but is more efficient and somewhat easier to use. As a very simply example, the following code creates a tibble containing respondents, their gender, and their height. data &lt;- tibble(resp = c(1,2,3), gender = c(&quot;M&quot;,&quot;F&quot;,&quot;F&quot;), height = c(176, 165, 172)) data 7.4.3 PRACTICE PROBLEM 4 How would you create a subset that contains only female participants? HINT: no need to use tidyverse, use the R base commands you learned earlier # SOLUTION 7.4.4 Reading data The example above manually created a data set, but in most cases you will start with data that you get from elsewhere, such as a csv file (e.g. downloaded from an online dataset or exported from excel) or an SPSS or Stata data file. Tidyverse contains a function read_csv that allows you to read a csv file directly into a tibble. You specify the location of the file, either on your local drive or directly from the Internet! The example below downloads an overview of gun polls from the data analytics site 538, and reads it into a tibble using the read_csv function. url &lt;- &quot;https://raw.githubusercontent.com/fivethirtyeight/data/master/poll-quiz-guns/guns-polls.csv&quot; d &lt;- read_csv(url) d (Note that you can safely ignore the (red) message, they simply tell you how each column was parsed) Calling the dataset (by simply typing in the name of it - d) shows the first ten rows of the dataset, and if the columns don’t fit they are not printed. The remaining rows and columns are printed at the bottom. For each column the data type is also mentioned (int stands for integer, which is a numeric value; chr is textual or character data). If you want to browse through your data, you can also click on the name of the data.frame (d) in the top-right window “Environment” tab or call View(d). 7.4.5 Subsetting with filter() The filter function can be used to select a subset of rows. In the guns data, the Question column specifies which question was asked. We can select only those rows (polls) that asked whether the minimum purchase age for guns should be raised to 21. age21 &lt;- filter(d, Question == &#39;age-21&#39;) age21 This call is typical for a tidyverse function: the first argument is the data to be used (d), and the remaining argument(s) contain information on what should be done to the data. Reminder Note the use of == for comparison: In R, = means assingment and == means equals. Other comparisons are e.g. &gt; (greather than), &lt;= (less than or equal) and != (not equal). You can also combine multiple conditions with logical (boolean) operators: &amp; (and), | or, and ! (not), and you can use parentheses like in mathematics. Using a comparison, we can find all surveys where support for raising the gun age was at least 80%. filter(d, Question == &#39;age-21&#39; &amp; Support &gt;= 80) Note that this command did not assign the result to an object, so the result is only displayed on the screen but not remembered. This can be a great way to quickly inspect your data, but if you want to continue analysing this subset you need to assign it to an object as above. 7.4.6 Selecting certain columns Where filter selects specific rows, select allows you to select specific columns. Most simply, we can name the columns that we want to retrieve in the particular order we want to retrieve them. select(age21, Population, Support, Pollster) We can also specify a range of columns, for example all columns from Support to Democratic Support. select(age21, Support:`Democratic Support`) Note the use of ‘backticks’ (reverse quotes) to specify the column name, as R does not normally allow spaces in names. You can also use some more versatile functions such as contains() or starts_with() within a select() command. select(age21, contains(&quot;Supp&quot;)) # Selects all variables that contain the stem &quot;Supp&quot; in their name Select can also be used to rename columns when selecting them, for example to get rid of the spaces. select(age21, Pollster, rep = `Republican Support`, dem = `Democratic Support`) Note that select drops all columns not selected. If you only want to rename columns, you can use the rename function (by first providing the new column name and then after the equal sign the old name). rename(age21, start_date = Start, end_date = End) Finally, you can drop a variable by adding a minus sign in front of a name: select(age21, -Question, -URL) 7.4.7 Sorting with arrange() You can easily sort a data set with arrange: you first specify the data, and then the column(s) to sort on. To sort in descending order, put a minus in front of a variable. For example, the following orders by population and then by support (descending). age21 &lt;- arrange(age21, Population, -Support) age21 Note that we assigned the result of arranging to the age21 object again, i.e. we replace the object by its sorted version. If we wouldn’t assign it to anything, it would display it on screen but not remember the sorting. Assigning a result to the same name means we don’t create a new object, preventing the environment from being cluttered (and saving us from the bother of thinking up yet another object name). For sorting, this should generally be fine as the sorted data should contain the same data as before. For subsetting, this means that the rows or columns are actually deleted from the dataset (in memory), so you will have to read the file again (or start from an earlier object) if you need those rows or columns later. 7.5 Adding or transforming variables with mutate() The mutate function makes it easy to create new variables or to modify existing ones. For those more familiar with SPSS, this is what you would do with compute and recode. If you look at the documentation page, you see that mutate works similarly to filter() and select(), in the sense that the first argument is the tibble, and then any number of additional arguments can be given to perform mutations. The mutations themselves are named arguments, in which you can provide any calculations using the existing columns. Here we’ll first create some variables and then look at the variables (using the select function to focus on the changes). Specifically, we’ll make a column for the absolute difference between the support scores for republicans and democrats, as a measure of how much they disagree. age21 &lt;- mutate(age21, party_diff = abs(`Republican Support` - `Democratic Support`)) select(age21, Question, Pollster, party_diff) age21 &lt;- arrange(age21, Population, -Support) To transform (recode) a variable in the same column, you can simply use an existing name in mutate() to overwrite it. 7.6 Working with Pipes If you look at the code above, you notice that the result of the mutate function is stored as an object, and that this object is used as the first argument for the next functions (select and arrange). This is a very common usage pattern, and it can be seen as a pipeline of functions, where the output of each function is the input for the next function. Because this is so common and usually we don’t really care about this temporary object but only about the final outcome, tidyverse offers a more convenient way of writing the code above using the pipeline operator %&gt;%. In short, whenever you write f(a, x) you can replace it by a %&gt;% f(x). If you then want to use the output of f(a, x) for a second function, you can just add it to the pipe: a %&gt;% f(x) %&gt;% f2(y) is equivalent to f2(f(a,x), y), or more readable, b=f(a,x); f2(b, y) Put simply, pipes take the output of a function, and directly use that output as the input for the .data argument in the next function. As you have seen, all the dplyr functions that we discussed have in common that the first argument is a tibble, and all functions return a tibble. This is intentional, and allows us to pipe all the functions together. This might seem a bit abstract, but consider the code below, which is a collection of statements from above: d &lt;- read_csv(url) age21 &lt;- filter(d, Question == &#39;age-21&#39;) age21 &lt;- mutate(age21, party_diff = abs(`Republican Support` - `Democratic Support`)) age21 &lt;- select(age21, Question, Pollster, party_diff) arrange(age21, -party_diff) To recap, this reads the csv, filters by question, computes the difference, drops other variables, and sorts. Since the output of each function is the input of the next, we can also write this as a single pipeline. read_csv(url) %&gt;% filter(Question == &#39;age-21&#39;) %&gt;% mutate(party_diff = abs(`Republican Support` - `Democratic Support`)) %&gt;% select(Question, Pollster, party_diff) %&gt;% arrange(-party_diff) The nice thing about pipes is that it makes it really clear what you are doing. Also, it doesn’t require making many intermediate objects. If applied right, piping allows you to make nicely contained pieces of code to perform specific parts of your analysis from raw input straight to results, including statistical modeling or visualization. It usually makes sense to have each “step” in the pipeline in its own line. This way, we can easily read the code line by line. Of course, you probably don’t want to replace your whole script with a single pipe, and often it is nice to store intermediate values. For example, you might want to download, clean, and subset a data set before doing multiple analyses with it. In that case, you probably want to store the result of downloading, cleaning, and subsetting as a variable, and use that in your analyses. 7.6.0.1 PRACTICE PROBLEM 5 Create a subset of the tibble d in which only polls with the question “arm-teacher” are included. Select only the variables Pollster, Population, and Support (feel free to rename them to more shorter abbreviations at the same time). Recode the variable Support so that it ranges from 0 to 1. Recode the variable Population so that the values reader reg and adu (tip: use the function recode() within a mutate() command. If you don’t know how the recode function works, use the help function ?!) # SOLUTION 7.6.1 Data Summarization The functions used in the earlier part on data preparation worked on individual rows. Sometimes, you need to compute properties of groups of rows (cases). This is called aggregation (or summarization) and in tidyverse uses the group_by function followed by either summarize or mutate. Let’s again work with the gun-poll data, remove the URL and rename some variables. d &lt;- d %&gt;% select(-URL) %&gt;% rename(Rep = `Republican Support`, Dem = `Democratic Support`) d 7.6.1.1 Grouping rows Now, we can use the group_by function to group by, for example, question. d %&gt;% group_by(Question) As you can see, the data itself didn’t actually change yet, it merely recorded (at the top) that we are now grouping by Question, and that there are 8 groups (different questions) in total. 7.6.1.2 Summarizing To summarize, you follow the group_by with a call to summarize. Summarize has a syntax that is similar to mutate: summarize(column = calculation, ...). The crucial difference, however, is that you always need to use a function in the calculation, and that function needs to compute a single summary value given a vector of values. Very common summarization functions are sum, mean, and sd (standard deviation). For example, the following computes the average support per question (and sorts by descending support). d %&gt;% group_by(Question) %&gt;% ## group by &quot;Questions&quot; summarize(Support = mean(Support)) %&gt;% ## average &quot;Support&quot; per group arrange(-Support) ## sort based on &quot;Support&quot; As you can see, summarize drastically changes the shape of the data. There are now rows equal to the number of groups (8), and the only columns left are the grouping variables and the summarized values. You can also compute summaries of multiple values, and even do ad hoc calculations. d %&gt;% group_by(Question) %&gt;% summarize(Dem = mean(Dem), Rep = mean(Rep), Diff = mean(Dem-Rep)) %&gt;% arrange(-Diff) So, Democrats are more in favor of all proposed gun laws except arming teachers. You can also compute multiple summaries of a single value. Another useful function is n() (without arguments), which simply counts the values in each group. For example, the following gives the count, mean, and standard deviation of the support. d %&gt;% group_by(Question) %&gt;% summarize(n = n(), mean = mean(Support), sd = sd(Support)) Note As you can see, one of the values has a missing value (NA) for standard deviation. Why? 7.6.1.3 Using mutate with group_by The examples above all reduce the number of cases to the number of groups. Another option is to use mutate after a group_by, which allows you to add summary values to the rows themselves. For example, suppose we wish to see whether a certain poll has a different prediction from the average polling of that question. We can group_by question and then use mutate to calculate the average support. d2 &lt;- d %&gt;% group_by(Question) %&gt;% mutate(avg_support = mean(Support), diff = Support - avg_support) d2 As you can see, where summarize reduces the rows and columns to the groups and summaries, mutate adds a new column which is identical for all rows within a group. 7.6.2 Ungrouping Finally, you can use ungroup to get rid of any groupings. For example, the data produced by the example above is still grouped by Question as mutate does not remove grouping information. So, if we want to compute the overall standard deviation of the difference we could ungroup and then summarize. d2 %&gt;% ungroup() %&gt;% summarize(diff = sd(diff)) (of course, running sd(d2$diff)) would yield the same result.) If you run the same command without the ungroup, what would the result be? Why? 7.6.2.1 Multiple grouping variables The above examples all used a single grouping variable, but you can also group by multiple columns. For example, we could compute average support per question and per population. d %&gt;% group_by(Question, Population) %&gt;% summarize(Support = mean(Support)) This results in a data set with one row per unique group, i.e. combination of Question and Population, and with separate columns for each grouping column and the summary values. 7.6.3 PRACTICE PROBLEM 6 The following data set stems from an experiment conducted by Masur, DiFranzo and Bazarova (2021) in which participants were exposed to social media feeds that differed with regard to how many of the (simulated) users showed themselves visually in the posts and profile pictures. The experiment a 3 (norm: 0%, 20%, 80% of the posts contained faces) x 3 (profile: 0%, 20%, 80% of the profile pictures contained faces) between-subject design. The authors were interested whether these manipulations led to different norm perceptions and different intentions to disclose oneself. The full data set can be downloaded here: https://osf.io/kfm6x/. This subset contains the following variables: id: participants’ unique identifiers condition: the condition they were randomly assigned. norm: The first manipulated factor, i.e., number of posts that contained faces. profile: The second manipulated factor, i.e., the number of profile pictures that contained faces age: Age of the participants -gender: Gender of the participants -norm_perc: A scale measuring how strongly participants perceived the social norm to disclose oneself on the platform. -disclosure: Their intention to disclose themselves on the platform Using the functions of the tidyverse, try to answer the following questions: On average, does age differ across the nine conditions? How did the two manipulations (norm and profile) affect subsequent norm perceptions and disclosure intentions? Think about how you can compute the difference across conditions for both factors independently as well as at the same time. Bonus question: How strongly are norm perceptions and intentions correlated (tip: try ?cor.test to learn how to compute correlations)? # SOLUTION d &lt;- read_csv(&quot;https://raw.githubusercontent.com/masurp/VU_CADC/main/tutorials/data/masur-et-al_data.csv&quot;) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
